# -*- coding: utf-8 -*-
"""Untitled34.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ax7fJnIQj7-klIp5Ar1N_G_HqN9u20uX
"""

!git clone https://github.com/ddan050/github_use_TEST.git

import torch
import torch.nn as nn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

!gdown --id 1Xa0MfTTID6NWC0y4dsQ0scn_gP2Eft0p
hansol_information = pd.read_csv('Chart_data_ver1.csv')

hansol_information

reverse_index = [i for i in range(hansol_information.shape[0]-1,-1,-1)]

hansol_information_reverse = pd.DataFrame(hansol_information, index=reverse_index)

hansol = hansol_information_reverse.reset_index(drop=True)

hansol['Datetime'] = pd.to_datetime(hansol['date'], format="%Y.%m.%d")

hansol = hansol.drop('date',axis=1)

hansol = hansol.set_index(keys=['Datetime'])

hansol

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))

time_steps = 20
for_periods = 10
train_data = hansol[:'2021']
test_data = hansol['2022']
train_data = train_data.to_numpy()
test_data = test_data.to_numpy()
x_train = []
y_train = []
y_train_stacked = []
train_len = len(train_data)
for i in range(time_steps, train_len-(for_periods)):
  x_train.append(train_data[i-time_steps:i,:]) 
for i in range(time_steps, train_len-for_periods):
  y_train.append(train_data[i:i+for_periods,3])
x_train,y_train = np.array(x_train), np.array(y_train)

nor_x_train = normalise_windows(x_train)
nor_y_train = normalise_windows(y_train)
nor_x_train = np.array(nor_x_train)
nor_y_train = np.array(nor_y_train)
nor_x_train = np.reshape(nor_x_train, (nor_x_train.shape[0], nor_x_train.shape[1],6))

inputs = pd.concat((hansol[:'2021'],hansol['2022']),axis=0)
inputs = inputs.to_numpy()
test_len = len(test_data)
inputs = inputs[len(inputs)-test_len-time_steps:] # 길이를 기준으로 예측날짜의 값으로 넘어가기

x_test = []
for i in range(time_steps, test_len+time_steps-for_periods):
  x_test.append(inputs[i-time_steps:i,:])
x_test = np.array(x_test)
nor_x_test = normalise_windows(x_test)
nor_x_test = np.array(nor_x_test)
nor_x_test = np.reshape(nor_x_test,(nor_x_test.shape[0],nor_x_test.shape[1],6))

def train_test_validation(all_data,time_steps, for_periods):
  train_data = all_data[:'2021']
  test_data = all_data['2022']
  #train_len = len(train_data)
  #test_len = len(test_data)

  from sklearn.preprocessing import MinMaxScaler
  scaler = MinMaxScaler(feature_range=(0,1))
  scaler = scaler.fit(train_data)
  trained_scaled = scaler.transform(train_data)
  print(trained_scaled.shape)
  test_scaled = scaler.transform(test_data)
  train_len = len(trained_scaled)
  test_len = len(test_scaled)

  x_train = []
  y_train = []
  y_train_stacked = []
  for i in range(time_steps, train_len-(for_periods)):
    x_train.append(trained_scaled[i-time_steps:i,:]) 
  for i in range(time_steps, train_len-for_periods):
    y_train.append(trained_scaled[i:i+for_periods,3])
  x_train,y_train = np.array(x_train), np.array(y_train)
  print(x_train.shape)
  print(y_train.shape)
  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],6)) # (sample,time step,feature)로 만듦

  #inputs = pd.concat((all_data[:'2021'],all_data['2022':]),axis=0).values # 예측값들은 train 대비 뒤에를 예측해야하는거니까 일단 하나로 모으고
  inputs = pd.concat((all_data[:'2021'],all_data['2022']),axis=0)
  inputs_scaled = scaler.transform(inputs)
  inputs_scaled = inputs_scaled[len(inputs_scaled)-len(test_scaled)-time_steps:] # 길이를 기준으로 예측날짜의 값으로 넘어가기

  x_test = []
  y_test = []
  for i in range(time_steps, test_len+time_steps-for_periods):
    x_test.append(inputs_scaled[i-time_steps:i,:])
  for i in range(time_steps, test_len+time_steps-for_periods):
    y_test.append(inputs_scaled[i:i+for_periods,3])
  x_test = np.array(x_test)
  y_test = np.array(y_test)
  print(x_test.shape)
  print(y_test.shape)
  x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],6))

  return x_train, y_train, x_test, y_test

from sklearn.preprocessing import MinMaxScaler
import numpy as np

class MinMaxScaler3D(MinMaxScaler):

    def fit_transform(self, X, y=None):
        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))
        return np.reshape(super().fit_transform(x, y=y), newshape=X.shape)

    def transform(self,X,y=None):
        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))
        return np.reshape(super().transform(x, y=y), newshape=X.shape)

def train_test_validation(all_data,time_steps, for_periods):
  train_data = all_data[:'2021']
  test_data = all_data['2022']
  #train_len = len(train_data)
  #test_len = len(test_data)

  arr_train_data = train_data.to_numpy()
  arr_test_data = test_data.to_numpy()

  train_len = len(arr_train_data)
  test_len = len(arr_test_data)

  inputs = pd.concat((all_data[:'2021'],all_data['2022']),axis=0)
  input_data = inputs.to_numpy()
  input_len = len(input_data)
  arr_test_data = arr_test_data[input_len-test_len-time_steps:]

  y_train = []
  y_test = []
  for i in range(time_steps, train_len-for_periods):
    y_train.append(arr_train_data[i:i+for_periods,3])
  for i in range(time_steps, test_len+time_steps-for_periods):
    y_test.append(input_data[i:i+for_periods,3])
  y_train = np.array(y_train)
  print(y_train.shape)
  y_test = np.array(y_test)
  print(y_test.shape)
  

  from sklearn.preprocessing import MinMaxScaler
  scaler = MinMaxScaler()
  trained_scaled = scaler.fit_transform(train_data)
  print(trained_scaled.shape)
  test_scaled = scaler.transform(inputs)

  x_train = []
  for i in range(time_steps, train_len-(for_periods)):
    x_train.append(trained_scaled[i-time_steps:i,:]) 
  x_train = np.array(x_train)
  print(x_train.shape)
  x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1],6)) # (sample,time step,feature)로 만듦

  x_test = []
  for i in range(time_steps, test_len+time_steps-for_periods):
    x_test.append(test_scaled[i-time_steps:i,:])
  x_test = np.array(x_test)
  print(x_test.shape)
  x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],6))


  #train_len = len(trained_scaled)
  #test_len = len(test_scaled)


  return x_train, y_train, x_test, y_test

X_Train, Y_Train, X_Test,Y_test = train_test_validation(hansol,20,5)

print(X_Train)

print(Y_Train[0])

from torch.autograd import Variable

class AIQ_LSTM(torch.nn.Module):
  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):
    super(AIQ_LSTM, self).__init__()
    self.num_classes = num_classes
    self.num_layers = num_layers
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.seq_length = seq_length
  
    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)
    self.fc_1 = nn.Linear(hidden_size, 128)
    self.fc = nn.Linear(128, num_classes)

    self.relu = nn.ReLU()

  def forward(self,x):
    x = torch.from_numpy(x).float()
    h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))
    c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))
    output,(hn,cn) = self.lstm(x,(h_0,c_0))

    hn = hn.view(-1, self.hidden_size)
    out = self.relu(hn)
    out = self.fc_1(out)
    out = self.relu(out)
    out = self.fc(out)

    return out

num_epochs= 10000
learning_rate = 0.0001

input_size = 6
hidden_size = 1
num_layers = 1

num_classes = 5
seq_length = 20

model = AIQ_LSTM(num_classes, input_size, hidden_size, num_layers, seq_length)

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

outputs_plot = np.zeros([num_epochs,3607])
y_plot = np.zeros([num_epochs,3607])
for epoch in range(num_epochs):
  outputs = model(X_Train)
  output_results = outputs.detach().numpy()
  outputs_plot[epoch,:]= np.mean(output_results,axis=1)
  optimizer.zero_grad()
  Y = Y_Train.astype(np.float32)
  Y = torch.from_numpy(Y)
  y_results = Y.detach().numpy()
  y_plot[epoch,:]= np.mean(y_results, axis =1)
  loss = criterion(outputs, Y)
  loss.backward()

  optimizer.step()
  if epoch % 100 == 0 :
    print("Epoch: %d, loss= %1.5f"%(epoch,loss.item()))

predict_output= model(X_Test)
predict_output = predict_output.detach().numpy()
predict_result = np.mean(predict_output,axis=1)

y = Y_test.astype(np.float32)
y = torch.from_numpy(y)
y_res = y.detach().numpy()
y_plot_test= np.mean(y_res, axis =1)

plt.plot(y_plot_test[0:60])
plt.show()

plt.plot(predict_result)

import matplotlib.pyplot as plt
plt.plot(outputs_plot[0])
plt.plot(y_plot[0])
plt.show()

plt.plot(outputs_plot[0])

plt.plot(outputs_plot[2500])
plt.plot(y_plot[2500])
plt.show()

plt.plot(outputs_plot[2500])

plt.plot(y_plot[9999])
plt.show()

plt.plot(outputs_plot[9999])

num_epochs= 10000
learning_rate = 0.001

input_size = 6
hidden_size = 1
num_layers = 1

num_classes = 10
seq_length = 20

model = AIQ_LSTM(num_classes, input_size, hidden_size, num_layers, seq_length)

criterion = torch.nn.L1Loss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

outputs_plot = np.zeros([num_epochs,3602])
y_plot = np.zeros([num_epochs,3602])
for epoch in range(num_epochs):
  outputs = model(nor_x_train)
  print("output",outputs)
  output_results = outputs.detach().numpy()
  outputs_plot[epoch,:]= np.mean(output_results,axis=1)
  optimizer.zero_grad()
  Y = nor_y_train.astype(np.float32)
  Y = torch.from_numpy(Y)
  y_results = Y.detach().numpy()
  y_plot[epoch,:]= np.mean(y_results, axis =1)
  loss = criterion(outputs, Y)
  loss.backward()

  optimizer.step()
  if epoch % 100 == 0 :
    print("Epoch: %d, loss= %1.5f"%(epoch,loss.item()))

nor_y_train

output_results

num_epochs= 10000
learning_rate = 0.0001

input_size = 6
hidden_size = 1
num_layers = 1

num_classes = 10
seq_length = 20

model = AIQ_LSTM(num_classes, input_size, hidden_size, num_layers, seq_length)

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

outputs_plot = np.zeros([num_epochs,3602])
profit = np.zeros([3602,10])
y_profit = np.zeros([3602,10])
output_profit = np.zeros([num_epochs,3602])
y_plot = np.zeros([num_epochs,3602])
y_output_profit = np.zeros([num_epochs,3602])

for epoch in range(num_epochs):
  outputs = model(X_Train)
  #print('outputs',outputs)
  output_results = outputs.detach().numpy()
  #print('output_results',output_results)
  for i in range (10):
    #print('calculate',(output_results[:,i]-output_results[:,0])/output_results[:,0])
    profit[:,i]= (output_results[:,i]-output_results[:,0])/output_results[:,0]
  #if epoch == 0 :
    #print('profit',profit)
  outputs_plot[epoch,:]= np.mean(output_results,axis=1)
  output_profit[epoch,:]= np.mean(profit)
  #if epoch == 0 :
    #print(output_profit[0])
  optimizer.zero_grad()
  Y = Y_Train.astype(np.float32)
  Y = torch.from_numpy(Y)
  y_results = Y.detach().numpy()
  for j in range (10):
    y_profit[:,j]= (y_results[:,j]-y_results[:,0])/y_results[:,0]
  y_plot[epoch,:]= np.mean(y_results, axis =1)
  #if epoch == 0 :
    #print(y_profit[0])
  y_output_profit[epoch,:] = np.mean(y_profit)
  #if epoch == 0 :
    #print(y_output_profit[0])
  Profit = torch.from_numpy(profit)
  #print(Profit)
  Y_profit = torch.from_numpy(y_profit)
  loss = criterion(outputs, Y)
  loss.backward()

  optimizer.step()
  if epoch % 100 == 0 :
    print("Epoch: %d, loss= %1.5f"%(epoch,loss.item()))

y_output_profit

plt.plot(output_profit[9999])

plt.show()

plt.plot(y_output_profit[9999,500:2500])
plt.plot(output_profit[9999,500:2500])
plt.show()

plt.plot(y_output_profit[9999,0:320])
plt.plot(output_profit[9999,0:320])
plt.show()

plt.plot(y_output_profit[9999,500:2500])
plt.show()

predict_profit=np.zeros([68,10])
predict_y_profit=np.zeros([68,10])
predict_output= model(X_Test)
predict_output = predict_output.detach().numpy()

predict_result = np.mean(predict_output,axis=1)
for i in range (10):
    predict_profit[:,i]= (predict_output[:,i]-predict_output[:,0])/predict_output[:,0]
predict_profit_plot= np.mean(predict_profit,axis=1)
y = Y_test.astype(np.float32)
y = torch.from_numpy(y)
y_res = y.detach().numpy()
y_plot_test= np.mean(y_res, axis =1)
for j in range (10):
    predict_y_profit[:,i]= (y_res[:,i]-y_res[:,0])/y_res[:,0]
predict_y_profit_plot = np.mean(predict_y_profit,axis=1)

plt.plot(predict_profit_plot)
plt.plot(predict_y_profit_plot[0:60])
plt.show()

def normalise_windows(window_data): # 첫번째 값을 기준으로 정규화해주는 방법 (첫번째 값 기준 상승/하락 비율)
  normalised_data = []
  for window in range(window_data.shape[0]):
      noramlising = []
      for p in range(window_data.shape[1]):
        noramlising.append(window_data[window,p] /window_data[window,0] - 1)
      normalised_data.append(noramlising)
  return normalised_data

